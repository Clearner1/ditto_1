import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoModel, AdamW, get_linear_schedule_with_warmup
from torch.utils.data import DataLoader
from tensorboardX import SummaryWriter
import numpy as np
import sklearn.metrics as metrics
import os
import time

from .layers import AttentionLayer, GlobalAttentionLayer, StructAttentionLayer

lm_mp = {
    'roberta': 'roberta-base',
    'distilbert': 'distilbert-base-uncased'
}

class DittoHGATModel(nn.Module):
    """Enhanced Ditto model with Hierarchical Graph Attention Networks"""
    
    def __init__(self, device='cuda', lm='roberta', alpha_aug=0.8, num_views=3):
        super().__init__()
        # Load pre-trained language model
        if lm in lm_mp:
            self.bert = AutoModel.from_pretrained(lm_mp[lm])
        else:
            self.bert = AutoModel.from_pretrained(lm)

        self.device = device
        self.alpha_aug = alpha_aug
        self.num_views = num_views

        # Get hidden size from BERT config
        hidden_size = self.bert.config.hidden_size

        # Initialize attention layers
        self.token_attention = AttentionLayer(hidden_size)
        self.global_attention = GlobalAttentionLayer(hidden_size)
        self.struct_attention = StructAttentionLayer(hidden_size)

        # Initialize view-specific layers
        self.view_fcs = nn.ModuleList([
            nn.Linear(hidden_size, hidden_size)
            for _ in range(num_views)
        ])

        # Final classification layer
        self.fc = nn.Linear(hidden_size, 2)

    def encode_sequence(self, input_ids, attention_mask=None):
        """Encode a sequence using BERT and attention mechanisms"""
        outputs = self.bert(input_ids, attention_mask=attention_mask)
        sequence_output = outputs[0]  # (batch_size, seq_len, hidden_size)
        cls_output = sequence_output[:, 0]  # (batch_size, hidden_size)

        # Apply different attention mechanisms
        token_repr = self.token_attention(sequence_output, attention_mask)
        global_repr = self.global_attention(sequence_output, attention_mask)
        
        # Create different views
        views = [
            cls_output,  # CLS token view
            token_repr,  # Token-level attention view
            global_repr  # Global attention view
        ]

        # Apply view-specific transformations
        views = [fc(view) for fc, view in zip(self.view_fcs, views)]

        # Combine views using structural attention
        combined_repr = self.struct_attention(views)
        
        return combined_repr

    def forward(self, x1, x2=None):
        """Forward pass with optional MixDA augmentation"""
        x1 = x1.to(self.device)
        
        if x2 is not None:
            # MixDA augmentation
            x2 = x2.to(self.device)
            
            # Get attention masks
            attention_mask1 = (x1 != self.bert.config.pad_token_id).float()
            attention_mask2 = (x2 != self.bert.config.pad_token_id).float()
            
            # Encode both sequences
            enc1 = self.encode_sequence(x1, attention_mask1)
            enc2 = self.encode_sequence(x2, attention_mask2)
            
            # Apply MixDA
            aug_lam = torch.tensor(
                np.random.beta(self.alpha_aug, self.alpha_aug),
                device=self.device
            )
            mixed_enc = enc1 * aug_lam + enc2 * (1.0 - aug_lam)
            return self.fc(mixed_enc)
        else:
            # Regular forward pass
            attention_mask = (x1 != self.bert.config.pad_token_id).float()
            enc = self.encode_sequence(x1, attention_mask)
            return self.fc(enc)


def train_epoch(model, iterator, optimizer, scheduler=None, hp=None):
    """Train the model for one epoch"""
    model.train()
    criterion = nn.CrossEntropyLoss()
    total_loss = 0
    
    for i, batch in enumerate(iterator):
        optimizer.zero_grad()
        
        # Get the inputs
        x1, x2 = batch[0], batch[1]
        labels = batch[-1].to(model.device)
        
        # Forward pass
        logits = model(x1, x2)
        loss = criterion(logits, labels)
        
        # Backward pass
        loss.backward()
        optimizer.step()
        if scheduler:
            scheduler.step()
            
        total_loss += loss.item()
        
        if hp and hp.fp16:
            with torch.cuda.amp.autocast():
                if i % 10 == 0:
                    print(f"step: {i}, loss: {loss.item():.4f}")
        else:
            if i % 10 == 0:
                print(f"step: {i}, loss: {loss.item():.4f}")
                
    return total_loss / len(iterator)


def evaluate(model, iterator):
    """Evaluate the model on a dataset"""
    model.eval()
    all_preds = []
    all_labels = []
    
    with torch.no_grad():
        for batch in iterator:
            x1 = batch[0]
            labels = batch[-1].to(model.device)
            
            logits = model(x1)
            preds = torch.argmax(logits, dim=1)
            
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    # Calculate metrics
    accuracy = metrics.accuracy_score(all_labels, all_preds)
    precision = metrics.precision_score(all_labels, all_preds)
    recall = metrics.recall_score(all_labels, all_preds)
    f1 = metrics.f1_score(all_labels, all_preds)
    
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1
    }


def train(trainset, validset, testset, run_tag, hp, device='cuda'):
    """Train and evaluate the model"""
    # Create data loaders with the custom collate function
    train_loader = DataLoader(
        trainset,
        batch_size=hp.batch_size,
        shuffle=True,
        collate_fn=trainset.pad,
        num_workers=0
    )
    valid_loader = DataLoader(
        validset,
        batch_size=hp.batch_size,
        shuffle=False,
        collate_fn=validset.pad,
        num_workers=0
    )
    test_loader = DataLoader(
        testset,
        batch_size=hp.batch_size,
        shuffle=False,
        collate_fn=testset.pad,
        num_workers=0
    )
    
    # Initialize model with attribute number from dataset
    model = DittoHGATModel(
        device=device,
        lm=hp.lm
    ).to(device)
    
    # Initialize optimizer with weight decay
    no_decay = ['bias', 'LayerNorm.weight']
    optimizer_grouped_parameters = [
        {
            'params': [p for n, p in model.named_parameters() 
                      if not any(nd in n for nd in no_decay)],
            'weight_decay': 0.01
        },
        {
            'params': [p for n, p in model.named_parameters() 
                      if any(nd in n for nd in no_decay)],
            'weight_decay': 0.0
        }
    ]
    optimizer = AdamW(optimizer_grouped_parameters, lr=hp.lr)
    
    # Initialize scheduler with warmup
    num_training_steps = (len(trainset) // hp.batch_size) * hp.n_epochs
    num_warmup_steps = num_training_steps // 10  # 10% of training steps for warmup
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=num_warmup_steps,
        num_training_steps=num_training_steps
    )
    
    # Initialize tensorboard
    writer = SummaryWriter(f'runs/{run_tag}')
    
    # Create checkpoint directory
    checkpoint_dir = os.path.join('checkpoints', run_tag)
    if not os.path.exists(checkpoint_dir):
        os.makedirs(checkpoint_dir)
    
    # Training loop
    best_dev_f1 = 0.0
    best_epoch = 0
    patience = 5  # Early stopping patience
    no_improvement = 0
    
    for epoch in range(hp.n_epochs):
        start_time = time.time()
        
        # Train for one epoch
        model.train()
        train_loss = train_epoch(model, train_loader, optimizer, scheduler, hp)
        
        # Evaluate
        model.eval()
        dev_metrics = evaluate(model, valid_loader)
        test_metrics = evaluate(model, test_loader)
        
        # Log metrics
        writer.add_scalar('train/loss', train_loss, epoch)
        writer.add_scalar('valid/f1', dev_metrics['f1'], epoch)
        writer.add_scalar('test/f1', test_metrics['f1'], epoch)
        
        # Save best model and check for early stopping
        if dev_metrics['f1'] > best_dev_f1:
            best_dev_f1 = dev_metrics['f1']
            best_epoch = epoch
            no_improvement = 0
            if hp.save_model:
                model_path = os.path.join(checkpoint_dir, 'model_best.pt')
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict(),
                    'best_f1': best_dev_f1,
                }, model_path)
        else:
            no_improvement += 1
            if no_improvement >= patience:
                print(f'Early stopping after {epoch + 1} epochs')
                break
        
        # Print progress
        print(f'Epoch {epoch+1}/{hp.n_epochs}:')
        print(f'  Train Loss: {train_loss:.4f}')
        print(f'  Valid F1: {dev_metrics["f1"]:.4f}')
        print(f'  Test F1: {test_metrics["f1"]:.4f}')
        print(f'  Time: {time.time() - start_time:.2f}s')
        print(f'  Best epoch: {best_epoch+1} with F1: {best_dev_f1:.4f}')
    
    # Load best model and evaluate on test set
    if hp.save_model:
        checkpoint = torch.load(os.path.join(checkpoint_dir, 'model_best.pt'))
        model.load_state_dict(checkpoint['model_state_dict'])
    
    model.eval()
    final_test_metrics = evaluate(model, test_loader)
    
    print('\nFinal Test Results:')
    print(f'  Best epoch: {best_epoch+1}')
    print(f'  Accuracy: {final_test_metrics["accuracy"]:.4f}')
    print(f'  Precision: {final_test_metrics["precision"]:.4f}')
    print(f'  Recall: {final_test_metrics["recall"]:.4f}')
    print(f'  F1: {final_test_metrics["f1"]:.4f}')
    
    writer.close()
    return final_test_metrics